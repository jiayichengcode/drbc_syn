{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "6d91e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from helper import *\n",
    "from calculate_delta import *\n",
    "import sys\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import os\n",
    "from drmv_riskfree import *\n",
    "\n",
    "#autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "ec163382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma_real = np.load('real_data_sigma.npy')\n",
    "sigma_real = 0.4*np.eye(20)\n",
    "df=pd.read_csv('df_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "5a449a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_mkt_data_highdim(T, num_paths, \n",
    "                         sigma, s0, dt=1/2520, seed=1):\n",
    "    \"\"\"\n",
    "    使用联合离散分布，模拟高维市场数据。\n",
    "\n",
    "    参数:\n",
    "        T (float): 总模拟时间 (例如，1.0 代表一年)。\n",
    "        joint_z_vectors (ndarray): 预定义的场景向量，形状为 (m, dim)。\n",
    "        p_dist (ndarray): 每个场景向量对应的概率，形状为 (m,)。\n",
    "        num_paths (int): 要模拟的路径数量。\n",
    "        sigma (ndarray): **波动率矩阵 σ**，形状为 (dim, dim)。\n",
    "        s0 (float): 初始价格。  \n",
    "        dt (float): 时间步长。\n",
    "\n",
    "    返回:\n",
    "        S (ndarray): 模拟的股价路径，形状 (num_paths, N+1, dim)。\n",
    "        t_list (ndarray): 时间点列表，形状 (N+1,)。\n",
    "        b_vectors (ndarray): 为每条路径选择的漂移向量，形状 (num_paths, dim)。\n",
    "        W (ndarray): 模拟的多维布朗运动，形状 (num_paths, N+1, dim)。\n",
    "    \"\"\"\n",
    "    dim = sigma.shape[0]\n",
    "    N = int(T / dt)  # 时间步数量\n",
    "    t_list = np.linspace(0, T, N + 1)\n",
    "    np.random.seed(seed)\n",
    "    # --- Bt=B0*(1+np.cos(2*np.pi*rand_k*t)) /2---\n",
    "    # 抽取 m 个场景的索引\n",
    "    # num_scenarios = joint_z_vectors.shape[0]\n",
    "    # scenario_indices = np.arange(num_scenarios)\n",
    "    # chosen_indices = np.random.choice(scenario_indices, p=p_dist, size=num_paths, replace=True)\n",
    "\n",
    "\n",
    "    B0=0.25\n",
    "    rand_k = np.random.normal(10, 20, sigma.shape[0]) # TODO: make k larger so fluctuate weekly or bi-weekly; can change to fixed numbers rather than random\n",
    "    # generate b_vectors, finally shape is (N, dim)\n",
    "    b_vectors = np.zeros((N, dim))\n",
    "    \n",
    "    # Create meshgrid for proper broadcasting: t (N,) and rand_k (dim,)\n",
    "    # We use t_list[:-1] to get N time steps (excluding the last one)\n",
    "    t_mesh, rand_k_mesh = np.meshgrid(t_list[:-1], rand_k, indexing='ij')\n",
    "    # Now t_mesh and rand_k_mesh both have shape (N, dim)\n",
    "    b_vectors = B0*(1 + 2*np.cos(2*np.pi*rand_k_mesh*t_mesh))/2\n",
    "\n",
    "    # --- 2. 模拟多维布朗运动 W ---\n",
    "    # 生成标准正态分布的增量\n",
    "    \n",
    "    normal_increments = np.random.normal(loc=0.0, scale=np.sqrt(dt), size=(num_paths, N, dim))\n",
    "    \n",
    "    W = np.zeros((num_paths, N + 1, dim))\n",
    "    # 通过对增量进行累积求和来构建布朗运动路径\n",
    "    W[:, 1:, :] = np.cumsum(normal_increments, axis=1)\n",
    "\n",
    "    # --- 3. 模拟股价路径 S ---\n",
    "    S = np.zeros((num_paths, N + 1, dim))\n",
    "\n",
    "    S[:, 0, :] = s0 * np.ones((num_paths, dim))\n",
    "\n",
    "    for i in range(N):\n",
    "        # 提取当前状态\n",
    "        current_S = S[:, i, :]\n",
    "        \n",
    "        # 布朗运动的增量 dW\n",
    "        dW = W[:, i + 1, :] - W[:, i, :]\n",
    "        \n",
    "        # --- 计算 SDE 的增量 dS ---\n",
    "        # 漂移项: b*dt\n",
    "        drift_term = b_vectors[i] * dt\n",
    "        \n",
    "        # 波动率项: σ * dW\n",
    "        # 使用矩阵乘法 (@)，并对 sigma 进行转置以匹配批量操作的维度\n",
    "        # (num_paths, dim) @ (dim, dim) -> (num_paths, dim)\n",
    "        vol_term = dW @ sigma.T\n",
    "        \n",
    "        # 逐元素乘法计算 dS\n",
    "        dS = current_S * (drift_term + vol_term)\n",
    "        \n",
    "        # 更新下一时间步的价格\n",
    "        S[:, i + 1, :] = current_S + dS\n",
    "    \n",
    "    # for each path, if any negative items in S, remove this path \n",
    "    for i in range(num_paths):\n",
    "        if np.any(S[i] < 0):\n",
    "            S = np.delete(S, i, axis=0)\n",
    "            W = np.delete(W, i, axis=0)\n",
    "    \n",
    "    # remove the first row of S\n",
    "    S = S[:, 1:, :]\n",
    "    W = W[:, 1:, :]\n",
    "    b_vectors = b_vectors[1:, :]\n",
    "    t_list = t_list[1:]\n",
    "    \n",
    "    return S, t_list, b_vectors, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "a2414119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3024, 20)"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_gen, t_list_gen, b_vectors_gen, W_gen = sim_mkt_data_highdim(T=0.12, num_paths=100, s0=10, sigma=sigma_real, dt=1/25200)\n",
    "prices_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "5bb56862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.66160523, 10.74065979,  8.71469559, 12.80173515, 11.99846425,\n",
       "        9.82017093, 10.79315395, 10.45533124, 12.52423954,  8.28799917,\n",
       "       11.27643314, 11.05741325, 11.10412199, 10.3397844 , 10.76321478,\n",
       "        9.18374774,  9.13087838,  7.90068316, 10.21003966, 11.11423352])"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_gen[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "808a43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data_to_df_hour(prices, real_trade_dates, types=120):\n",
    "    \"\"\"\n",
    "    Converts a 2D numpy array of intraday prices into two DataFrames: one for\n",
    "    daily aggregated data and one for intraday data. Assumes 10 observations per day.\n",
    "    Removes entire days for a stock if any intraday return is NaN to ensure consistency.\n",
    "\n",
    "    Args:\n",
    "        prices (np.ndarray): A 2D numpy array of shape (T, dim), where T is the\n",
    "                             number of time periods (days * 10) and dim is the\n",
    "                             number of stocks.\n",
    "        real_trade_dates (list or array): A list of trade dates.\n",
    "        types (int): The number of types to cycle through for the 'type' column.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:\n",
    "                                           - df_daily: Aggregated daily data.\n",
    "                                           - df_intraday: Intraday data with an 'hour' column.\n",
    "    \"\"\"\n",
    "    T, dim = prices.shape\n",
    "\n",
    "    if T % 10 != 0:\n",
    "        raise ValueError(\"The total number of time periods (T) must be a multiple of 10.\")\n",
    "    \n",
    "    num_days = T // 10\n",
    "    \n",
    "    if len(real_trade_dates) < num_days:\n",
    "        raise ValueError(\"Not enough real_trade_dates for the given price data.\")\n",
    "    \n",
    "    daily_dates = real_trade_dates[-num_days:]\n",
    "    \n",
    "    permnos = range(1, dim + 1)\n",
    "\n",
    "    # --- Create Intraday DataFrame ---\n",
    "    df_prc_wide = pd.DataFrame(prices, columns=permnos)\n",
    "    df_prc_wide['date'] = np.repeat(daily_dates, 10)\n",
    "    df_prc_wide['hour'] = np.tile(range(1, 11), num_days)\n",
    "    \n",
    "    df_intraday = df_prc_wide.melt(id_vars=['date', 'hour'], value_name='prc', var_name='permno')\n",
    "    \n",
    "    df_intraday.sort_values(['permno', 'date', 'hour'], inplace=True)\n",
    "    df_intraday.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Calculate returns. The first entry for each permno will be NaN.\n",
    "    df_intraday['log_ret'] = df_intraday.groupby('permno')['prc'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    df_intraday['ret'] = df_intraday.groupby('permno')['prc'].transform(pd.Series.pct_change)\n",
    "    \n",
    "    # --- Filter out entire days that contain any NaN returns ---\n",
    "    # Identify the (permno, date) pairs that have at least one NaN value\n",
    "    bad_days = df_intraday[df_intraday['log_ret'].isnull()][['permno', 'date']].drop_duplicates()\n",
    "    \n",
    "    if not bad_days.empty:\n",
    "        # Use a merge with an indicator to perform an anti-join, keeping only rows\n",
    "        # that are not in the 'bad_days' DataFrame.\n",
    "        df_intraday = df_intraday.merge(bad_days, on=['permno', 'date'], how='left', indicator=True)\n",
    "        df_intraday = df_intraday[df_intraday['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    df_intraday.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add remaining columns now that the data is clean\n",
    "    df_intraday['type'] = (df_intraday.groupby(['date', 'hour']).ngroup() % types) + 1\n",
    "    df_intraday['prc_adjusted'] = df_intraday['prc']\n",
    "\n",
    "    # Reorder columns for the intraday dataframe\n",
    "    df_intraday = df_intraday[['date', 'hour', 'permno', 'ret', 'prc', 'type', 'prc_adjusted', 'log_ret']]\n",
    "\n",
    "    # --- Create Daily DataFrame from the cleaned intraday data ---\n",
    "    daily_groups = df_intraday.groupby(['date', 'permno'])\n",
    "\n",
    "    df_daily = daily_groups.agg(\n",
    "        log_ret=('log_ret', 'sum'),\n",
    "        prc=('prc', 'last')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate daily simple return from aggregated log return\n",
    "    df_daily['ret'] = np.exp(df_daily['log_ret']) - 1\n",
    "    \n",
    "    # 'type' in daily data varies only with date\n",
    "    df_daily['type'] = (df_daily.groupby('date').ngroup() % types) + 1\n",
    "    df_daily['prc_adjusted'] = df_daily['prc']\n",
    "    \n",
    "    # Reorder columns for the daily dataframe\n",
    "    df_daily = df_daily[['date', 'permno', 'ret', 'prc', 'type', 'prc_adjusted', 'log_ret']]\n",
    "\n",
    "    return df_daily, df_intraday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "e471a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data_to_df(prices, real_trade_dates, types=120):\n",
    "    \"\"\"\n",
    "    Converts a 2D numpy array of prices into a long-format pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        prices (np.ndarray): A 2D numpy array of shape (T, dim), where T is the\n",
    "                             number of time periods and dim is the number of stocks.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns: 'date', 'permno', 'ret', and 'prc'.\n",
    "                      'permno' is the stock identifier, from 1 to dim.\n",
    "    \"\"\"\n",
    "    T, dim = prices.shape\n",
    "    dates = real_trade_dates[-T:]\n",
    "    permnos = range(1, dim + 1)\n",
    "\n",
    "    # Create a wide DataFrame for prices\n",
    "    df_prc = pd.DataFrame(prices, index=dates, columns=permnos)\n",
    "    df_prc.index.name = 'date'\n",
    "    df_prc.columns.name = 'permno'\n",
    "\n",
    "    # Calculate returns\n",
    "    df_ret = df_prc.pct_change()\n",
    "\n",
    "    # Stack prices and returns to convert to long format\n",
    "    # dropna=False is important to keep all price entries, even with NaN returns for the first day\n",
    "    s_prc = df_prc.stack(dropna=False).rename('prc')\n",
    "    s_ret = df_ret.stack(dropna=False).rename('ret')\n",
    "\n",
    "    # Combine into a single DataFrame, aligning on the (date, permno) index\n",
    "    df = pd.concat([s_ret, s_prc], axis=1)\n",
    "\n",
    "    # Reset index to get 'date' and 'permno' as columns\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Reorder columns to the desired format\n",
    "    df = df[['date', 'permno', 'ret', 'prc']]\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['type'] = (df.groupby('date').ngroup() % types) + 1\n",
    "    df['prc_adjusted'] = df['prc']\n",
    "    df['log_ret'] = df.groupby('permno')['prc'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "25ca7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = sim_data_to_df(prices_gen[0], df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "5a62aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>ret</th>\n",
       "      <th>prc</th>\n",
       "      <th>type</th>\n",
       "      <th>prc_adjusted</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001886</td>\n",
       "      <td>9.948944</td>\n",
       "      <td>2</td>\n",
       "      <td>9.948944</td>\n",
       "      <td>-0.001888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>10.038451</td>\n",
       "      <td>2</td>\n",
       "      <td>10.038451</td>\n",
       "      <td>0.003167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>10.017225</td>\n",
       "      <td>2</td>\n",
       "      <td>10.017225</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>10.048257</td>\n",
       "      <td>2</td>\n",
       "      <td>10.048257</td>\n",
       "      <td>-0.000737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>10.036769</td>\n",
       "      <td>2</td>\n",
       "      <td>10.036769</td>\n",
       "      <td>0.001245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60435</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>16</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>9.183748</td>\n",
       "      <td>23</td>\n",
       "      <td>9.183748</td>\n",
       "      <td>0.005167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60436</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>17</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>9.130878</td>\n",
       "      <td>23</td>\n",
       "      <td>9.130878</td>\n",
       "      <td>0.002730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60437</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.002143</td>\n",
       "      <td>7.900683</td>\n",
       "      <td>23</td>\n",
       "      <td>7.900683</td>\n",
       "      <td>-0.002145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60438</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>10.210040</td>\n",
       "      <td>23</td>\n",
       "      <td>10.210040</td>\n",
       "      <td>-0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60439</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>11.114234</td>\n",
       "      <td>23</td>\n",
       "      <td>11.114234</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60440 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  permno       ret        prc  type  prc_adjusted   log_ret\n",
       "0      2012-12-28       1 -0.001886   9.948944     2      9.948944 -0.001888\n",
       "1      2012-12-28       2  0.003172  10.038451     2     10.038451  0.003167\n",
       "2      2012-12-28       3  0.001307  10.017225     2     10.017225  0.001306\n",
       "3      2012-12-28       4 -0.000736  10.048257     2     10.048257 -0.000737\n",
       "4      2012-12-28       5  0.001246  10.036769     2     10.036769  0.001245\n",
       "...           ...     ...       ...        ...   ...           ...       ...\n",
       "60435  2024-12-31      16  0.005180   9.183748    23      9.183748  0.005167\n",
       "60436  2024-12-31      17  0.002734   9.130878    23      9.130878  0.002730\n",
       "60437  2024-12-31      18 -0.002143   7.900683    23      7.900683 -0.002145\n",
       "60438  2024-12-31      19 -0.001444  10.210040    23     10.210040 -0.001445\n",
       "60439  2024-12-31      20  0.000044  11.114234    23     11.114234  0.000044\n",
       "\n",
       "[60440 rows x 7 columns]"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "eb9a68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_day, df_intraday = sim_data_to_df_hour(prices_gen[0], df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aab903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sim_new_hour(daily_df, intraday_df, r=0.02, seed=42, start_date='2024-01-01', end_date='2024-12-31',\n",
    "                 beta=-3, num_stocks=20, plan_time=1/12, dt = 1/2520, sigma_real = sigma_real):\n",
    "    # Step 1: Load data\n",
    "    \n",
    "    df = daily_df\n",
    "    df_intra = intraday_df\n",
    "    # Sort by permno and date to ensure proper ordering for log return calculation\n",
    "    df = df.sort_values(['permno', 'date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_intra['date'] = pd.to_datetime(df_intra['date'])\n",
    "    # Calculate log returns for each stock\n",
    "    \n",
    "    \n",
    "    # Step 1: Find stocks with complete data from 2005-12-31 to 2015-01-01\n",
    "    initial_start = pd.to_datetime(start_date) - relativedelta(years=10)    \n",
    "    initial_end = pd.to_datetime(start_date) - pd.Timedelta(days=1)\n",
    "    \n",
    "    # Get stocks that have data in the initial period\n",
    "    initial_period_data = df[(df['date'] >= initial_start) & (df['date'] <= initial_end)]\n",
    "    \n",
    "    # Count trading days in the initial period for validation\n",
    "    total_trading_days = initial_period_data['date'].nunique()\n",
    "    # print(f\"Total trading days in initial period: {total_trading_days}\")\n",
    "    \n",
    "    # Find stocks with sufficient data coverage (at least 80% of trading days)\n",
    "    stock_coverage = initial_period_data.groupby('permno')['date'].nunique()\n",
    "    min_required_days = int(total_trading_days)  # Require at least 80% coverage\n",
    "    valid_stocks_initial = stock_coverage[stock_coverage >= min_required_days].index.tolist()\n",
    "    \n",
    "    # print(f\"Stocks with sufficient data in initial period: {len(valid_stocks_initial)}\")\n",
    "    \n",
    "    # Sample num_stocks stocks from those with complete initial data\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "    selected_stocks = np.sort(np.random.choice(valid_stocks_initial, num_stocks, replace=False))\n",
    "    \n",
    "    print(f\"Initially selected stocks: {selected_stocks}\")\n",
    "    \n",
    "    # Step 2: Process monthly data starting from 2015-01-01\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    month_starts = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    \n",
    "    current_stocks = selected_stocks.copy()\n",
    "    kara_wealth_list = [1]\n",
    "    drbc_wealth_list = [1]\n",
    "    drmv_wealth_list = [1]\n",
    "    for i, current_month in enumerate(tqdm(month_starts)):\n",
    "        # print(f\"\\nProcessing month {i+1}/{len(month_starts)}: {current_month}\")\n",
    "        \n",
    "        # Define time windows\n",
    "        train_start = current_month - relativedelta(years=10)\n",
    "        train_end = current_month - pd.Timedelta(days=1)\n",
    "        test_start = current_month\n",
    "        test_end = month_starts[i+1] - pd.Timedelta(days=1) if i+1 < len(month_starts) else pd.to_datetime(end_date)\n",
    "        \n",
    "        # print(f\"Previous 10 years: {train_start.date()} to {train_end.date()}\")\n",
    "        # print(f\"Next month: {test_start.date()} to {test_end.date()}\")\n",
    "        \n",
    "        prev_to_next_dates = df[(df['date'] >= train_start) & (df['date'] <= test_end)]['date'].nunique()\n",
    "        \n",
    "        # Efficiently find all stocks with 100% coverage using vectorized operations\n",
    "        # Get data for the entire period (train + test)\n",
    "        full_period_start = train_start\n",
    "        full_period_end = test_end\n",
    "        full_period_data = df[(df['date'] >= full_period_start) & (df['date'] <= full_period_end)]\n",
    "        full_period_stock_dates = full_period_data.groupby('permno')['date'].nunique()\n",
    "        all_valid_stocks = full_period_stock_dates[full_period_stock_dates >= prev_to_next_dates].index.values\n",
    "        \n",
    "        # Check which current stocks are still valid\n",
    "        valid_current_stocks = np.intersect1d(current_stocks, all_valid_stocks)\n",
    "        \n",
    "        # print(f\"Current stocks with 100% coverage: {len(valid_current_stocks)} out of {len(current_stocks)}\")\n",
    "        # print(f\"Total stocks available with 100% coverage: {len(all_valid_stocks)}\")\n",
    "        \n",
    "        # If we need to replace stocks to maintain 20 stocks\n",
    "        stocks_needed = num_stocks - len(valid_current_stocks)\n",
    "        \n",
    "        if stocks_needed > 0:\n",
    "            # print(f\"Need to find {stocks_needed} replacement stocks\")\n",
    "            \n",
    "            # Find replacement candidates (exclude currently valid stocks)\n",
    "            replacement_candidates = np.setdiff1d(all_valid_stocks, valid_current_stocks)\n",
    "            \n",
    "            # print(f\"Available replacement candidates: {len(replacement_candidates)}\")\n",
    "            # add to 20 stocks\n",
    "            stocks_to_add = np.random.choice(replacement_candidates, stocks_needed, replace=False)\n",
    "            # Use all available replacements, even if less than needed\n",
    "            current_stocks = np.sort(np.concatenate([valid_current_stocks, stocks_to_add]))\n",
    "\n",
    "        else:\n",
    "            current_stocks = valid_current_stocks\n",
    "            # print(\"All current stocks are valid, no replacement needed\")\n",
    "        \n",
    "        # print(f\"Final stock selection for this month: {current_stocks}\")\n",
    "        # print(f\"Number of stocks: {len(current_stocks)}\")\n",
    "        \n",
    "        # Get training data for the selected stocks\n",
    "        pretrain_data = df[(df['date'] >= train_start) & (df['date'] <= train_end) & \n",
    "                          (df['permno'].isin(current_stocks))]\n",
    "        drmv_weights = run_single_backtest_select_stocks(\n",
    "            training_data=pretrain_data,\n",
    "            selected_perms=current_stocks,\n",
    "            annual_target_return=0.105,\n",
    "            r=r)\n",
    "        length = len(pretrain_data) / len(current_stocks)\n",
    "        prev_sigma_start_dt = (train_start - relativedelta(months=1))\n",
    "        #prev_sigma_start_dt = (train_start - relativedelta(years=1))\n",
    "        to_get_B_data_intraday = df_intra[(df_intra['date'] >= prev_sigma_start_dt) & (df_intra['date'] <= train_end) & \n",
    "                          (df_intra['permno'].isin(current_stocks))]\n",
    "        matrix, n_to_average = compute_annualized_matrix_type(to_get_B_data_intraday, sigma_real, dt=dt)\n",
    "        ret_matrix = pretrain_data.pivot(index='date', columns='permno', values='ret')\n",
    "        ret_matrix = ret_matrix.fillna(0)\n",
    "        \n",
    "       \n",
    "        # use real sigma matrix (already annualized)\n",
    "        sigma_mat = sigma_real #np.linalg.cholesky(cov)\n",
    "        curr_data = df[(df['date'] <= test_end) & (df['date'] >= current_month)&(df['permno'].isin(current_stocks))]\n",
    "        curr_data_intra = df_intra[(df_intra['date'] <= test_end) & (df_intra['date'] >= current_month)&(df_intra['permno'].isin(current_stocks))]\n",
    "        # dt = 1/length\n",
    "        t_list = np.linspace(0, 1, int(1/dt))\n",
    "        price_st = curr_data.pivot(index='date', columns='permno', values='prc_adjusted').fillna(method='ffill').values\n",
    "        price_st_intra = curr_data_intra.pivot(index=['date', 'hour'], columns='permno', values='prc_adjusted').fillna(method='ffill').values\n",
    "        curr_all_ret = price_st[-1] / price_st[0] - 1\n",
    "        yt = St_to_Yt_vectorized(price_st_intra[np.newaxis, :, :], price_st_intra[0], sigma_mat, r, t_list[1:int(len(curr_data_intra)/num_stocks)+1]) # can be t_list[0:len(curr_data)]\n",
    "        k= solve_k_with_EL(matrix, r=r, sigma=sigma_mat, T=plan_time, beta=beta, num_y=1000, seed=seed)\n",
    "\n",
    "        # calculate radius small delta (using 1 year, represents by T=1)\n",
    "        var = calculate_z_var(T=plan_time, r=r, sigma=sigma_mat, B_support=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)\n",
    "        np.random.seed(seed)\n",
    "        small_delta_array = (np.random.normal(0, np.sqrt(var), size=100)**2)*(calculate_numerator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)/calculate_denominator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed))\n",
    "        small_delta = np.percentile(small_delta_array, 95)/n_to_average\n",
    "        \n",
    "        # calculate delta_B (using 1 year, represents by T=1)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        delta_B = compute_big_delta_star(matrix, r, plan_time, beta, small_delta, sigma_mat, rng=rng)\n",
    "        \n",
    "        month_r = np.power(1+r, 1/12)-1 \n",
    "        # add month_r to curr_all_ret for drmv\n",
    "        curr_ret_for_drmv = np.append(curr_all_ret, month_r)\n",
    "        daily_kara = 1\n",
    "        daily_drbc = 1\n",
    "        granular_r = np.power(1+r, dt)-1\n",
    "        # last day not trade since no price for next day\n",
    "        for j in range(1,int((curr_data['date'].nunique()/dt)/252)):\n",
    "            kara_frac_daily = pi_fraction_exact(t=j*dt, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat,\n",
    "                        joint_z_vectors=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            drbc_frac_daily = pi_fraction_exact(t=j*dt, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat, \n",
    "                        joint_z_vectors=matrix+delta_B, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            daily_kara *= (1-kara_frac_daily.sum())*granular_r+np.dot(kara_frac_daily, price_st_intra[j] / price_st_intra[j-1] - 1)+1\n",
    "            daily_drbc *= (1-drbc_frac_daily.sum())*granular_r+np.dot(drbc_frac_daily, price_st_intra[j] / price_st_intra[j-1] - 1)+1\n",
    "            \n",
    "        # kara_wealth_list.append((kara_wealth_list[-1]*(1-kara_frac.sum())*month_r+np.dot(kara_frac, curr_all_ret)+1)*kara_wealth_list[-1])\n",
    "        # drbc_wealth_list.append((drbc_wealth_list[-1]*(1-drbc_frac.sum())*month_r+np.dot(drbc_frac, curr_all_ret)+1)*drbc_wealth_list[-1])\n",
    "        drmv_wealth_list.append(drmv_wealth_list[-1]*(1+np.dot(drmv_weights, curr_ret_for_drmv)))\n",
    "        kara_wealth_list.append(daily_kara*kara_wealth_list[-1])\n",
    "        drbc_wealth_list.append(daily_drbc*drbc_wealth_list[-1])\n",
    "\n",
    "    return kara_wealth_list, drbc_wealth_list, drmv_wealth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c71b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = main_sim_new_hour(daily_df=df_day, intraday_df=df_intraday)\n",
    "for i in range(len(a[0])):\n",
    "    print(a[0][i], a[1][i], a[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "f83cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sim_new(input_df, r=0.02, seed=42, start_date='2024-01-01', end_date='2024-12-31',\n",
    "                 beta=-3, num_stocks=20, plan_time=1/120,dt=1/2520):\n",
    "    # Step 1: Load data\n",
    "    \n",
    "    df = input_df\n",
    "    \n",
    "    # Sort by permno and date to ensure proper ordering for log return calculation\n",
    "    df = df.sort_values(['permno', 'date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Calculate log returns for each stock\n",
    "    \n",
    "    \n",
    "    # Step 1: Find stocks with complete data from 2005-12-31 to 2015-01-01\n",
    "    initial_start = pd.to_datetime(start_date) - relativedelta(years=10)    \n",
    "    initial_end = pd.to_datetime(start_date) - pd.Timedelta(days=1)\n",
    "    \n",
    "    # Get stocks that have data in the initial period\n",
    "    initial_period_data = df[(df['date'] >= initial_start) & (df['date'] <= initial_end)]\n",
    "    \n",
    "    # Count trading days in the initial period for validation\n",
    "    total_trading_days = initial_period_data['date'].nunique()\n",
    "    # print(f\"Total trading days in initial period: {total_trading_days}\")\n",
    "    \n",
    "    # Find stocks with sufficient data coverage (at least 80% of trading days)\n",
    "    stock_coverage = initial_period_data.groupby('permno')['date'].nunique()\n",
    "    min_required_days = int(total_trading_days)  # Require at least 80% coverage\n",
    "    valid_stocks_initial = stock_coverage[stock_coverage >= min_required_days].index.tolist()\n",
    "    \n",
    "    # print(f\"Stocks with sufficient data in initial period: {len(valid_stocks_initial)}\")\n",
    "    \n",
    "    # Sample num_stocks stocks from those with complete initial data\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "    selected_stocks = np.sort(np.random.choice(valid_stocks_initial, num_stocks, replace=False))\n",
    "    \n",
    "    print(f\"Initially selected stocks: {selected_stocks}\")\n",
    "    \n",
    "    # Step 2: Process monthly data starting from 2015-01-01\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    month_starts = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    \n",
    "    current_stocks = selected_stocks.copy()\n",
    "    kara_wealth_list = [1]\n",
    "    drbc_wealth_list = [1]\n",
    "    drmvrf_wealth_list = [1]\n",
    "    drmv_wealth_list = [1]\n",
    "    for i, current_month in enumerate(tqdm(month_starts)):\n",
    "        # print(f\"\\nProcessing month {i+1}/{len(month_starts)}: {current_month}\")\n",
    "        \n",
    "        # Define time windows\n",
    "        train_start = current_month - relativedelta(years=10)\n",
    "        train_end = current_month - pd.Timedelta(days=1)\n",
    "        test_start = current_month\n",
    "        test_end = month_starts[i+1] - pd.Timedelta(days=1) if i+1 < len(month_starts) else pd.to_datetime(end_date)\n",
    "        \n",
    "        # print(f\"Previous 10 years: {train_start.date()} to {train_end.date()}\")\n",
    "        # print(f\"Next month: {test_start.date()} to {test_end.date()}\")\n",
    "        \n",
    "        prev_to_next_dates = df[(df['date'] >= train_start) & (df['date'] <= test_end)]['date'].nunique()\n",
    "        \n",
    "        # Efficiently find all stocks with 100% coverage using vectorized operations\n",
    "        # Get data for the entire period (train + test)\n",
    "        full_period_start = train_start\n",
    "        full_period_end = test_end\n",
    "        full_period_data = df[(df['date'] >= full_period_start) & (df['date'] <= full_period_end)]\n",
    "        full_period_stock_dates = full_period_data.groupby('permno')['date'].nunique()\n",
    "        all_valid_stocks = full_period_stock_dates[full_period_stock_dates >= prev_to_next_dates].index.values\n",
    "        \n",
    "        # Check which current stocks are still valid\n",
    "        valid_current_stocks = np.intersect1d(current_stocks, all_valid_stocks)\n",
    "        \n",
    "        # print(f\"Current stocks with 100% coverage: {len(valid_current_stocks)} out of {len(current_stocks)}\")\n",
    "        # print(f\"Total stocks available with 100% coverage: {len(all_valid_stocks)}\")\n",
    "        \n",
    "        # If we need to replace stocks to maintain 20 stocks\n",
    "        stocks_needed = num_stocks - len(valid_current_stocks)\n",
    "        \n",
    "        if stocks_needed > 0:\n",
    "            # print(f\"Need to find {stocks_needed} replacement stocks\")\n",
    "            \n",
    "            # Find replacement candidates (exclude currently valid stocks)\n",
    "            replacement_candidates = np.setdiff1d(all_valid_stocks, valid_current_stocks)\n",
    "            \n",
    "            # print(f\"Available replacement candidates: {len(replacement_candidates)}\")\n",
    "            # add to 20 stocks\n",
    "            stocks_to_add = np.random.choice(replacement_candidates, stocks_needed, replace=False)\n",
    "            # Use all available replacements, even if less than needed\n",
    "            current_stocks = np.sort(np.concatenate([valid_current_stocks, stocks_to_add]))\n",
    "\n",
    "        else:\n",
    "            current_stocks = valid_current_stocks\n",
    "            # print(\"All current stocks are valid, no replacement needed\")\n",
    "        \n",
    "        # print(f\"Final stock selection for this month: {current_stocks}\")\n",
    "        # print(f\"Number of stocks: {len(current_stocks)}\")\n",
    "        \n",
    "        # Get training data for the selected stocks\n",
    "        pretrain_data = df[(df['date'] >= train_start) & (df['date'] <= train_end) & \n",
    "                          (df['permno'].isin(current_stocks))]\n",
    "        pretrain_data_drmv = pretrain_data.copy()\n",
    "        pretrain_data_drmv['ret'] = (pretrain_data_drmv['ret']/dt)/252\n",
    "        drmv_weights_rf = run_single_backtest_select_stocks(\n",
    "            training_data=pretrain_data_drmv,\n",
    "            selected_perms=current_stocks,\n",
    "            annual_target_return=0.105,\n",
    "            r=r)\n",
    "        drmv_weights = run_single_backtest_select_stocks(\n",
    "            training_data=pretrain_data_drmv,\n",
    "            selected_perms=current_stocks,\n",
    "            annual_target_return=0.105,\n",
    "            r=r, riskfree=False)\n",
    "        length = len(pretrain_data) / len(current_stocks)\n",
    "        prev_sigma_start_dt = (train_start - relativedelta(months=1))\n",
    "        #prev_sigma_start_dt = (train_start - relativedelta(years=1))\n",
    "        to_get_B_data = df[(df['date'] >= prev_sigma_start_dt) & (df['date'] <= train_end) & \n",
    "                          (df['permno'].isin(current_stocks))]\n",
    "        matrix, _ = compute_annualized_matrix_type(to_get_B_data, sigma_real, dt=dt)\n",
    "        #matrix = matrix/10\n",
    "        #matrix = np.random.normal(r, 0.1, (30, 20))\n",
    "        # for dt=1/2520 ang T=1.2 case\n",
    "        #matrix = matrix*10\n",
    "        # ret_matrix = pretrain_data.pivot(index='date', columns='permno', values='ret')\n",
    "        # ret_matrix = ret_matrix.fillna(0)\n",
    "        \n",
    "       \n",
    "        # # use real sigma matrix (already annualized)\n",
    "        # lw = LedoitWolf(store_precision=False, assume_centered=True)\n",
    "        # lw.fit(ret_matrix)\n",
    "        # cov = lw.covariance_ * 252\n",
    "        sigma_mat = sigma_real #np.linalg.cholesky(cov) #sigma_real/9 # \n",
    "        curr_data = df[(df['date'] <= test_end) & (df['date'] >= current_month)&(df['permno'].isin(current_stocks))]\n",
    "        dt = 1/length\n",
    "        t_list = np.linspace(0, 1, int(1/dt))\n",
    "        price_st = curr_data.pivot(index='date', columns='permno', values='prc_adjusted').fillna(method='ffill').values\n",
    "        curr_all_ret = price_st[-1] / price_st[0] - 1\n",
    "        yt = St_to_Yt_vectorized(price_st[np.newaxis, :, :], price_st[0], sigma_mat, r, t_list[1:int(len(curr_data)/20)+1]) # can be t_list[0:len(curr_data)]\n",
    "        k= solve_k_with_EL(matrix, r=r, sigma=sigma_mat, T=plan_time, beta=beta, num_y=1000, seed=seed)\n",
    "\n",
    "        # calculate radius small delta (using 1 year, represents by T=1)\n",
    "        var = calculate_z_var(T=plan_time, r=r, sigma=sigma_mat, B_support=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)\n",
    "        small_delta_array = (np.random.normal(0, np.sqrt(var), size=100)**2)/calculate_numerator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)  #/calculate_denominator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed))\n",
    "        # calculate_denominator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)\n",
    "        # calculate_numerator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)\n",
    "        small_delta = np.percentile(small_delta_array, 98)/100\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # calculate delta_B (using 1 year, represents by T=1)\n",
    "        delta_B = compute_big_delta_star(matrix, r, plan_time, beta, small_delta, sigma_mat, rng=rng)\n",
    "        \n",
    "        month_r = np.power(1+r, (1/12)/(dt*252))-1 \n",
    "        # add month_r to curr_all_ret for drmv\n",
    "        curr_ret_for_drmv = np.append(curr_all_ret, month_r)\n",
    "        daily_kara = 1\n",
    "        daily_drbc = 1\n",
    "        daily_r = np.power(1+r, (1/252)/(dt*252))-1\n",
    "        # last day not trade since no price for next day\n",
    "        for j in range(1,curr_data['date'].nunique()):\n",
    "            kara_frac_daily = pi_fraction_exact(t=j*dt, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat,\n",
    "                        joint_z_vectors=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            drbc_frac_daily = pi_fraction_exact(t=j*dt, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat, \n",
    "                        joint_z_vectors=matrix+delta_B, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            daily_kara *= (1-kara_frac_daily.sum())*daily_r+np.dot(kara_frac_daily, price_st[j] / price_st[j-1] - 1)+1\n",
    "            daily_drbc *= (1-drbc_frac_daily.sum())*daily_r+np.dot(drbc_frac_daily, price_st[j] / price_st[j-1] - 1)+1\n",
    "            \n",
    "            \n",
    "        # kara_wealth_list.append((kara_wealth_list[-1]*(1-kara_frac.sum())*month_r+np.dot(kara_frac, curr_all_ret)+1)*kara_wealth_list[-1])\n",
    "        # drbc_wealth_list.append((drbc_wealth_list[-1]*(1-drbc_frac.sum())*month_r+np.dot(drbc_frac, curr_all_ret)+1)*drbc_wealth_list[-1])\n",
    "        drmvrf_wealth_list.append(drmvrf_wealth_list[-1]*(1+np.dot(drmv_weights_rf, curr_ret_for_drmv)))\n",
    "        drmv_wealth_list.append(drmv_wealth_list[-1]*(1+np.dot(drmv_weights, curr_ret_for_drmv[0:len(drmv_weights)])))\n",
    "        kara_wealth_list.append(daily_kara*kara_wealth_list[-1])\n",
    "        drbc_wealth_list.append(daily_drbc*drbc_wealth_list[-1])\n",
    "\n",
    "    return kara_wealth_list, drbc_wealth_list, drmv_wealth_list, drmvrf_wealth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "88e37f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially selected stocks: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:43<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "b = main_sim_new(sim_df,seed=50, dt=1/25200, plan_time=1/1200, beta=-3, r=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "a89b9206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1\n",
      "1.3511084161775244 1.3482547186960452 1.0034783146912507 1.005971633536195\n",
      "1.367062459206597 1.363715009541502 1.0067400879750832 1.0104833766048469\n",
      "1.365340136459935 1.3641840000311227 1.0032513623724801 1.0081996023624047\n",
      "1.3782924295612393 1.3793804644537426 1.0057373443029207 1.0113135720964828\n",
      "1.3714707975358396 1.3744823403193074 1.0044630743032268 1.0109281115301394\n",
      "1.3799607071095268 1.3822401520493186 1.008118580712116 1.0153327896582016\n",
      "1.4032843063404121 1.4075071230316871 1.010885020490001 1.0190769796870662\n",
      "1.418685258520184 1.4238335683943169 1.0101703471092751 1.0199755741585033\n",
      "1.451191228077101 1.4583604403162462 1.0090420609812532 1.0193018784505898\n",
      "1.4684525942371955 1.480627029396805 1.0093747165496985 1.0203082006256818\n",
      "1.4798494241984237 1.4940624812593135 1.0144404852769424 1.0250710739177966\n",
      "1.4798641537155561 1.498820975903845 1.0103062678264836 1.0225722053132522\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(b[0])):\n",
    "    print(b[0][i], b[1][i], b[2][i], b[3][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "0a3b7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.44355442076643 13.983387518333284 10.10886277962771 23.115238299235035\n"
     ]
    }
   ],
   "source": [
    "def sharpe_ratio(wealths, risk_free_rate=0.03):\n",
    "    returns = np.diff(wealths, axis=0) / wealths[:-1] - 0.01*risk_free_rate/(len(wealths)-1)\n",
    "    return np.mean(returns) * np.sqrt((len(wealths)-1)*100) / np.std(returns)\n",
    "\n",
    "print(sharpe_ratio(b[0]), sharpe_ratio(b[1]), sharpe_ratio(b[2]), sharpe_ratio(b[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(-2.200894145436558), np.float64(9.561147663547718), np.float64(1.8943288472613853)]\n",
      "[np.float64(2.3117558736459403), np.float64(-5.5267476105814515), np.float64(3.283226865064755)]\n",
      "[np.float64(7.163147301537872), np.float64(1.5202116640620973), np.float64(-0.7755380485326381)]\n",
      "[np.float64(-0.050408984138445065), np.float64(-1.2104733166160375), np.float64(-1.5782987170808143)]\n",
      "[np.float64(-22.100792969314885), np.float64(-10.368163022428275), np.float64(4.403584322826358)]\n",
      "[np.float64(-3.979891933902924), np.float64(17.523504421217865), np.float64(-0.33425435804273723)]\n",
      "[np.float64(6.8616089567079035), np.float64(-10.33660070528853), np.float64(7.749747300356541)]\n",
      "[np.float64(3.9907309225900285), np.float64(4.004573348274443), np.float64(-1.5046925953654555)]\n",
      "[np.float64(1.7916758663159846), np.float64(6.28345929794088), np.float64(-1.9123754621549605)]\n",
      "[np.float64(-10.97061174690924), np.float64(-19.69778628563763), np.float64(5.539727420695167)]\n",
      "[np.float64(0.7291258644318636), np.float64(1.6150155748407051), np.float64(0.6749590056920047)]\n",
      "[np.float64(-10.656497949187337), np.float64(6.308163900729725), np.float64(4.696143020909986)]\n"
     ]
    }
   ],
   "source": [
    "# get all the pkl files in the res folder\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "pkl_files = [f for f in os.listdir('res') if f.endswith('.pkl')]\n",
    "\n",
    "# load all the pkl files\n",
    "sharpe = []\n",
    "for pkl_file in pkl_files:\n",
    "    with open(f'res/{pkl_file}', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(data['sharpe_list'])\n",
    "    sharpe.append(data['sharpe_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc347e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.25925441, -0.02697459,  1.84471313])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sharpe, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de670e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

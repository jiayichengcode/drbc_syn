{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d91e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from helper import *\n",
    "from calculate_delta import *\n",
    "import sys\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import os\n",
    "from drmv_riskfree import *\n",
    "\n",
    "#autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec163382",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_real = np.load('real_data_sigma.npy')\n",
    "df=pd.read_csv('df_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5a449a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_mkt_data_highdim(T, num_paths, \n",
    "                         sigma, s0, dt=1/2520, seed=1):\n",
    "    \"\"\"\n",
    "    使用联合离散分布，模拟高维市场数据。\n",
    "\n",
    "    参数:\n",
    "        T (float): 总模拟时间 (例如，1.0 代表一年)。\n",
    "        joint_z_vectors (ndarray): 预定义的场景向量，形状为 (m, dim)。\n",
    "        p_dist (ndarray): 每个场景向量对应的概率，形状为 (m,)。\n",
    "        num_paths (int): 要模拟的路径数量。\n",
    "        sigma (ndarray): **波动率矩阵 σ**，形状为 (dim, dim)。\n",
    "        s0 (float): 初始价格。  \n",
    "        dt (float): 时间步长。\n",
    "\n",
    "    返回:\n",
    "        S (ndarray): 模拟的股价路径，形状 (num_paths, N+1, dim)。\n",
    "        t_list (ndarray): 时间点列表，形状 (N+1,)。\n",
    "        b_vectors (ndarray): 为每条路径选择的漂移向量，形状 (num_paths, dim)。\n",
    "        W (ndarray): 模拟的多维布朗运动，形状 (num_paths, N+1, dim)。\n",
    "    \"\"\"\n",
    "    dim = sigma.shape[0]\n",
    "    N = int(T / dt)  # 时间步数量\n",
    "    t_list = np.linspace(0, T, N + 1)\n",
    "    np.random.seed(seed)\n",
    "    # --- Bt=B0*(1+np.cos(2*np.pi*rand_k*t)) /2---\n",
    "    # 抽取 m 个场景的索引\n",
    "    # num_scenarios = joint_z_vectors.shape[0]\n",
    "    # scenario_indices = np.arange(num_scenarios)\n",
    "    # chosen_indices = np.random.choice(scenario_indices, p=p_dist, size=num_paths, replace=True)\n",
    "\n",
    "\n",
    "    B0=0.1\n",
    "    rand_k = np.random.normal(10, 30, sigma.shape[0]) # TODO: make k larger so fluctuate weekly or bi-weekly; can change to fixed numbers rather than random\n",
    "    # generate b_vectors, finally shape is (N, dim)\n",
    "    b_vectors = np.zeros((N, dim))\n",
    "    \n",
    "    # Create meshgrid for proper broadcasting: t (N,) and rand_k (dim,)\n",
    "    # We use t_list[:-1] to get N time steps (excluding the last one)\n",
    "    t_mesh, rand_k_mesh = np.meshgrid(t_list[:-1], rand_k, indexing='ij')\n",
    "    # Now t_mesh and rand_k_mesh both have shape (N, dim)\n",
    "    b_vectors = B0*(1 + 2*np.cos(2*np.pi*rand_k_mesh*t_mesh))/2\n",
    "\n",
    "    # --- 2. 模拟多维布朗运动 W ---\n",
    "    # 生成标准正态分布的增量\n",
    "    \n",
    "    normal_increments = np.random.normal(loc=0.0, scale=np.sqrt(dt), size=(num_paths, N, dim))\n",
    "    \n",
    "    W = np.zeros((num_paths, N + 1, dim))\n",
    "    # 通过对增量进行累积求和来构建布朗运动路径\n",
    "    W[:, 1:, :] = np.cumsum(normal_increments, axis=1)\n",
    "\n",
    "    # --- 3. 模拟股价路径 S ---\n",
    "    S = np.zeros((num_paths, N + 1, dim))\n",
    "\n",
    "    S[:, 0, :] = s0 * np.ones((num_paths, dim))\n",
    "\n",
    "    for i in range(N):\n",
    "        # 提取当前状态\n",
    "        current_S = S[:, i, :]\n",
    "        \n",
    "        # 布朗运动的增量 dW\n",
    "        dW = W[:, i + 1, :] - W[:, i, :]\n",
    "        \n",
    "        # --- 计算 SDE 的增量 dS ---\n",
    "        # 漂移项: b*dt\n",
    "        drift_term = b_vectors[i] * dt\n",
    "        \n",
    "        # 波动率项: σ * dW\n",
    "        # 使用矩阵乘法 (@)，并对 sigma 进行转置以匹配批量操作的维度\n",
    "        # (num_paths, dim) @ (dim, dim) -> (num_paths, dim)\n",
    "        vol_term = dW @ sigma.T\n",
    "        \n",
    "        # 逐元素乘法计算 dS\n",
    "        dS = current_S * (drift_term + vol_term)\n",
    "        \n",
    "        # 更新下一时间步的价格\n",
    "        S[:, i + 1, :] = current_S + dS\n",
    "    \n",
    "    # for each path, if any negative items in S, remove this path \n",
    "    for i in range(num_paths):\n",
    "        if np.any(S[i] < 0):\n",
    "            S = np.delete(S, i, axis=0)\n",
    "            W = np.delete(W, i, axis=0)\n",
    "    \n",
    "    # remove the first row of S\n",
    "    S = S[:, 1:, :]\n",
    "    W = W[:, 1:, :]\n",
    "    b_vectors = b_vectors[1:, :]\n",
    "    t_list = t_list[1:]\n",
    "    \n",
    "    return S, t_list, b_vectors, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a2414119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3024, 20)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_gen, t_list_gen, b_vectors_gen, W_gen = sim_mkt_data_highdim(T=12, num_paths=100, s0=10, sigma=sigma_real/8, dt=1/252)\n",
    "prices_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "808a43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data_to_df_hour(prices, real_trade_dates, types=120):\n",
    "    \"\"\"\n",
    "    Converts a 2D numpy array of intraday prices into two DataFrames: one for\n",
    "    daily aggregated data and one for intraday data. Assumes 10 observations per day.\n",
    "    Removes entire days for a stock if any intraday return is NaN to ensure consistency.\n",
    "\n",
    "    Args:\n",
    "        prices (np.ndarray): A 2D numpy array of shape (T, dim), where T is the\n",
    "                             number of time periods (days * 10) and dim is the\n",
    "                             number of stocks.\n",
    "        real_trade_dates (list or array): A list of trade dates.\n",
    "        types (int): The number of types to cycle through for the 'type' column.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:\n",
    "                                           - df_daily: Aggregated daily data.\n",
    "                                           - df_intraday: Intraday data with an 'hour' column.\n",
    "    \"\"\"\n",
    "    T, dim = prices.shape\n",
    "\n",
    "    if T % 10 != 0:\n",
    "        raise ValueError(\"The total number of time periods (T) must be a multiple of 10.\")\n",
    "    \n",
    "    num_days = T // 10\n",
    "    \n",
    "    if len(real_trade_dates) < num_days:\n",
    "        raise ValueError(\"Not enough real_trade_dates for the given price data.\")\n",
    "    \n",
    "    daily_dates = real_trade_dates[-num_days:]\n",
    "    \n",
    "    permnos = range(1, dim + 1)\n",
    "\n",
    "    # --- Create Intraday DataFrame ---\n",
    "    df_prc_wide = pd.DataFrame(prices, columns=permnos)\n",
    "    df_prc_wide['date'] = np.repeat(daily_dates, 10)\n",
    "    df_prc_wide['hour'] = np.tile(range(1, 11), num_days)\n",
    "    \n",
    "    df_intraday = df_prc_wide.melt(id_vars=['date', 'hour'], value_name='prc', var_name='permno')\n",
    "    \n",
    "    df_intraday.sort_values(['permno', 'date', 'hour'], inplace=True)\n",
    "    df_intraday.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Calculate returns. The first entry for each permno will be NaN.\n",
    "    df_intraday['log_ret'] = df_intraday.groupby('permno')['prc'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    df_intraday['ret'] = df_intraday.groupby('permno')['prc'].transform(pd.Series.pct_change)\n",
    "    \n",
    "    # --- Filter out entire days that contain any NaN returns ---\n",
    "    # Identify the (permno, date) pairs that have at least one NaN value\n",
    "    bad_days = df_intraday[df_intraday['log_ret'].isnull()][['permno', 'date']].drop_duplicates()\n",
    "    \n",
    "    if not bad_days.empty:\n",
    "        # Use a merge with an indicator to perform an anti-join, keeping only rows\n",
    "        # that are not in the 'bad_days' DataFrame.\n",
    "        df_intraday = df_intraday.merge(bad_days, on=['permno', 'date'], how='left', indicator=True)\n",
    "        df_intraday = df_intraday[df_intraday['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    df_intraday.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add remaining columns now that the data is clean\n",
    "    df_intraday['type'] = (df_intraday.groupby(['date', 'hour']).ngroup() % types) + 1\n",
    "    df_intraday['prc_adjusted'] = df_intraday['prc']\n",
    "\n",
    "    # Reorder columns for the intraday dataframe\n",
    "    df_intraday = df_intraday[['date', 'hour', 'permno', 'ret', 'prc', 'type', 'prc_adjusted', 'log_ret']]\n",
    "\n",
    "    # --- Create Daily DataFrame from the cleaned intraday data ---\n",
    "    daily_groups = df_intraday.groupby(['date', 'permno'])\n",
    "\n",
    "    df_daily = daily_groups.agg(\n",
    "        log_ret=('log_ret', 'sum'),\n",
    "        prc=('prc', 'last')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate daily simple return from aggregated log return\n",
    "    df_daily['ret'] = np.exp(df_daily['log_ret']) - 1\n",
    "    \n",
    "    # 'type' in daily data varies only with date\n",
    "    df_daily['type'] = (df_daily.groupby('date').ngroup() % types) + 1\n",
    "    df_daily['prc_adjusted'] = df_daily['prc']\n",
    "    \n",
    "    # Reorder columns for the daily dataframe\n",
    "    df_daily = df_daily[['date', 'permno', 'ret', 'prc', 'type', 'prc_adjusted', 'log_ret']]\n",
    "\n",
    "    return df_daily, df_intraday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e471a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data_to_df(prices, real_trade_dates, types=60):\n",
    "    \"\"\"\n",
    "    Converts a 2D numpy array of prices into a long-format pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        prices (np.ndarray): A 2D numpy array of shape (T, dim), where T is the\n",
    "                             number of time periods and dim is the number of stocks.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns: 'date', 'permno', 'ret', and 'prc'.\n",
    "                      'permno' is the stock identifier, from 1 to dim.\n",
    "    \"\"\"\n",
    "    T, dim = prices.shape\n",
    "    dates = real_trade_dates[-T:]\n",
    "    permnos = range(1, dim + 1)\n",
    "\n",
    "    # Create a wide DataFrame for prices\n",
    "    df_prc = pd.DataFrame(prices, index=dates, columns=permnos)\n",
    "    df_prc.index.name = 'date'\n",
    "    df_prc.columns.name = 'permno'\n",
    "\n",
    "    # Calculate returns\n",
    "    df_ret = df_prc.pct_change()\n",
    "\n",
    "    # Stack prices and returns to convert to long format\n",
    "    # dropna=False is important to keep all price entries, even with NaN returns for the first day\n",
    "    s_prc = df_prc.stack(dropna=False).rename('prc')\n",
    "    s_ret = df_ret.stack(dropna=False).rename('ret')\n",
    "\n",
    "    # Combine into a single DataFrame, aligning on the (date, permno) index\n",
    "    df = pd.concat([s_ret, s_prc], axis=1)\n",
    "\n",
    "    # Reset index to get 'date' and 'permno' as columns\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Reorder columns to the desired format\n",
    "    df = df[['date', 'permno', 'ret', 'prc']]\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['type'] = (df.groupby('date').ngroup() % types) + 1\n",
    "    df['prc_adjusted'] = df['prc']\n",
    "    df['log_ret'] = df.groupby('permno')['prc'].transform(lambda x: np.log(x / x.shift(1)))\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "25ca7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = sim_data_to_df(prices_gen[0], df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5a62aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>ret</th>\n",
       "      <th>prc</th>\n",
       "      <th>type</th>\n",
       "      <th>prc_adjusted</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>9.961324</td>\n",
       "      <td>2</td>\n",
       "      <td>9.961324</td>\n",
       "      <td>-0.001856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>10.020765</td>\n",
       "      <td>2</td>\n",
       "      <td>10.020765</td>\n",
       "      <td>0.002451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>10.001678</td>\n",
       "      <td>2</td>\n",
       "      <td>10.001678</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000451</td>\n",
       "      <td>10.041058</td>\n",
       "      <td>2</td>\n",
       "      <td>10.041058</td>\n",
       "      <td>-0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>10.028686</td>\n",
       "      <td>2</td>\n",
       "      <td>10.028686</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60435</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>20.099963</td>\n",
       "      <td>23</td>\n",
       "      <td>20.099963</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60436</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.001238</td>\n",
       "      <td>19.517087</td>\n",
       "      <td>23</td>\n",
       "      <td>19.517087</td>\n",
       "      <td>-0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60437</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>16.583438</td>\n",
       "      <td>23</td>\n",
       "      <td>16.583438</td>\n",
       "      <td>-0.002313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60438</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.002434</td>\n",
       "      <td>19.503014</td>\n",
       "      <td>23</td>\n",
       "      <td>19.503014</td>\n",
       "      <td>-0.002437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60439</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>20.699808</td>\n",
       "      <td>23</td>\n",
       "      <td>20.699808</td>\n",
       "      <td>-0.002214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60440 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  permno       ret        prc  type  prc_adjusted   log_ret\n",
       "0      2012-12-28       1 -0.001855   9.961324     2      9.961324 -0.001856\n",
       "1      2012-12-28       2  0.002454  10.020765     2     10.020765  0.002451\n",
       "2      2012-12-28       3  0.000673  10.001678     2     10.001678  0.000672\n",
       "3      2012-12-28       4 -0.000451  10.041058     2     10.041058 -0.000451\n",
       "4      2012-12-28       5  0.000719  10.028686     2     10.028686  0.000719\n",
       "...           ...     ...       ...        ...   ...           ...       ...\n",
       "60435  2024-12-31      16  0.001073  20.099963    23     20.099963  0.001073\n",
       "60436  2024-12-31      17 -0.001238  19.517087    23     19.517087 -0.001239\n",
       "60437  2024-12-31      18 -0.002310  16.583438    23     16.583438 -0.002313\n",
       "60438  2024-12-31      19 -0.002434  19.503014    23     19.503014 -0.002437\n",
       "60439  2024-12-31      20 -0.002212  20.699808    23     20.699808 -0.002214\n",
       "\n",
       "[60440 rows x 7 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day, df_intraday = sim_data_to_df_hour(prices_gen[0], df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4a0c40b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>permno</th>\n",
       "      <th>ret</th>\n",
       "      <th>prc</th>\n",
       "      <th>type</th>\n",
       "      <th>prc_adjusted</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>9.815373</td>\n",
       "      <td>1</td>\n",
       "      <td>9.815373</td>\n",
       "      <td>0.009036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>9.788424</td>\n",
       "      <td>2</td>\n",
       "      <td>9.788424</td>\n",
       "      <td>-0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>9.794475</td>\n",
       "      <td>3</td>\n",
       "      <td>9.794475</td>\n",
       "      <td>0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004353</td>\n",
       "      <td>9.751844</td>\n",
       "      <td>4</td>\n",
       "      <td>9.751844</td>\n",
       "      <td>-0.004362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>9.864367</td>\n",
       "      <td>5</td>\n",
       "      <td>9.864367</td>\n",
       "      <td>0.011473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604595</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.002554</td>\n",
       "      <td>14.026441</td>\n",
       "      <td>106</td>\n",
       "      <td>14.026441</td>\n",
       "      <td>-0.002557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604596</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.003372</td>\n",
       "      <td>13.979145</td>\n",
       "      <td>107</td>\n",
       "      <td>13.979145</td>\n",
       "      <td>-0.003378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604597</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.009203</td>\n",
       "      <td>14.107792</td>\n",
       "      <td>108</td>\n",
       "      <td>14.107792</td>\n",
       "      <td>0.009161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604598</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>14.095090</td>\n",
       "      <td>109</td>\n",
       "      <td>14.095090</td>\n",
       "      <td>-0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604599</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.005399</td>\n",
       "      <td>14.018997</td>\n",
       "      <td>110</td>\n",
       "      <td>14.018997</td>\n",
       "      <td>-0.005413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604600 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  hour permno       ret        prc  type  prc_adjusted  \\\n",
       "0       2012-12-27     1      1  0.009077   9.815373     1      9.815373   \n",
       "1       2012-12-27     2      1 -0.002746   9.788424     2      9.788424   \n",
       "2       2012-12-27     3      1  0.000618   9.794475     3      9.794475   \n",
       "3       2012-12-27     4      1 -0.004353   9.751844     4      9.751844   \n",
       "4       2012-12-27     5      1  0.011539   9.864367     5      9.864367   \n",
       "...            ...   ...    ...       ...        ...   ...           ...   \n",
       "604595  2024-12-31     6     20 -0.002554  14.026441   106     14.026441   \n",
       "604596  2024-12-31     7     20 -0.003372  13.979145   107     13.979145   \n",
       "604597  2024-12-31     8     20  0.009203  14.107792   108     14.107792   \n",
       "604598  2024-12-31     9     20 -0.000900  14.095090   109     14.095090   \n",
       "604599  2024-12-31    10     20 -0.005399  14.018997   110     14.018997   \n",
       "\n",
       "         log_ret  \n",
       "0       0.009036  \n",
       "1      -0.002749  \n",
       "2       0.000618  \n",
       "3      -0.004362  \n",
       "4       0.011473  \n",
       "...          ...  \n",
       "604595 -0.002557  \n",
       "604596 -0.003378  \n",
       "604597  0.009161  \n",
       "604598 -0.000901  \n",
       "604599 -0.005413  \n",
       "\n",
       "[604600 rows x 8 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_intraday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aab903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sim_new_hour(daily_df, intraday_df, r=0.02, seed=42, start_date='2024-01-01', end_date='2024-12-31',\n",
    "                 beta=-3, num_stocks=20, plan_time=1/12, dt = 1/2520, sigma_real = sigma_real):\n",
    "    # Step 1: Load data\n",
    "    \n",
    "    df = daily_df\n",
    "    df_intra = intraday_df\n",
    "    # Sort by permno and date to ensure proper ordering for log return calculation\n",
    "    df = df.sort_values(['permno', 'date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_intra['date'] = pd.to_datetime(df_intra['date'])\n",
    "    # Calculate log returns for each stock\n",
    "    \n",
    "    \n",
    "    # Step 1: Find stocks with complete data from 2005-12-31 to 2015-01-01\n",
    "    initial_start = pd.to_datetime(start_date) - relativedelta(years=10)    \n",
    "    initial_end = pd.to_datetime(start_date) - pd.Timedelta(days=1)\n",
    "    \n",
    "    # Get stocks that have data in the initial period\n",
    "    initial_period_data = df[(df['date'] >= initial_start) & (df['date'] <= initial_end)]\n",
    "    \n",
    "    # Count trading days in the initial period for validation\n",
    "    total_trading_days = initial_period_data['date'].nunique()\n",
    "    # print(f\"Total trading days in initial period: {total_trading_days}\")\n",
    "    \n",
    "    # Find stocks with sufficient data coverage (at least 80% of trading days)\n",
    "    stock_coverage = initial_period_data.groupby('permno')['date'].nunique()\n",
    "    min_required_days = int(total_trading_days)  # Require at least 80% coverage\n",
    "    valid_stocks_initial = stock_coverage[stock_coverage >= min_required_days].index.tolist()\n",
    "    \n",
    "    # print(f\"Stocks with sufficient data in initial period: {len(valid_stocks_initial)}\")\n",
    "    \n",
    "    # Sample num_stocks stocks from those with complete initial data\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "    selected_stocks = np.sort(np.random.choice(valid_stocks_initial, num_stocks, replace=False))\n",
    "    \n",
    "    print(f\"Initially selected stocks: {selected_stocks}\")\n",
    "    \n",
    "    # Step 2: Process monthly data starting from 2015-01-01\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    month_starts = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    \n",
    "    current_stocks = selected_stocks.copy()\n",
    "    kara_wealth_list = [1]\n",
    "    drbc_wealth_list = [1]\n",
    "    drmv_wealth_list = [1]\n",
    "    for i, current_month in enumerate(tqdm(month_starts)):\n",
    "        # print(f\"\\nProcessing month {i+1}/{len(month_starts)}: {current_month}\")\n",
    "        \n",
    "        # Define time windows\n",
    "        train_start = current_month - relativedelta(years=10)\n",
    "        train_end = current_month - pd.Timedelta(days=1)\n",
    "        test_start = current_month\n",
    "        test_end = month_starts[i+1] - pd.Timedelta(days=1) if i+1 < len(month_starts) else pd.to_datetime(end_date)\n",
    "        \n",
    "        # print(f\"Previous 10 years: {train_start.date()} to {train_end.date()}\")\n",
    "        # print(f\"Next month: {test_start.date()} to {test_end.date()}\")\n",
    "        \n",
    "        prev_to_next_dates = df[(df['date'] >= train_start) & (df['date'] <= test_end)]['date'].nunique()\n",
    "        \n",
    "        # Efficiently find all stocks with 100% coverage using vectorized operations\n",
    "        # Get data for the entire period (train + test)\n",
    "        full_period_start = train_start\n",
    "        full_period_end = test_end\n",
    "        full_period_data = df[(df['date'] >= full_period_start) & (df['date'] <= full_period_end)]\n",
    "        full_period_stock_dates = full_period_data.groupby('permno')['date'].nunique()\n",
    "        all_valid_stocks = full_period_stock_dates[full_period_stock_dates >= prev_to_next_dates].index.values\n",
    "        \n",
    "        # Check which current stocks are still valid\n",
    "        valid_current_stocks = np.intersect1d(current_stocks, all_valid_stocks)\n",
    "        \n",
    "        # print(f\"Current stocks with 100% coverage: {len(valid_current_stocks)} out of {len(current_stocks)}\")\n",
    "        # print(f\"Total stocks available with 100% coverage: {len(all_valid_stocks)}\")\n",
    "        \n",
    "        # If we need to replace stocks to maintain 20 stocks\n",
    "        stocks_needed = num_stocks - len(valid_current_stocks)\n",
    "        \n",
    "        if stocks_needed > 0:\n",
    "            # print(f\"Need to find {stocks_needed} replacement stocks\")\n",
    "            \n",
    "            # Find replacement candidates (exclude currently valid stocks)\n",
    "            replacement_candidates = np.setdiff1d(all_valid_stocks, valid_current_stocks)\n",
    "            \n",
    "            # print(f\"Available replacement candidates: {len(replacement_candidates)}\")\n",
    "            # add to 20 stocks\n",
    "            stocks_to_add = np.random.choice(replacement_candidates, stocks_needed, replace=False)\n",
    "            # Use all available replacements, even if less than needed\n",
    "            current_stocks = np.sort(np.concatenate([valid_current_stocks, stocks_to_add]))\n",
    "\n",
    "        else:\n",
    "            current_stocks = valid_current_stocks\n",
    "            # print(\"All current stocks are valid, no replacement needed\")\n",
    "        \n",
    "        # print(f\"Final stock selection for this month: {current_stocks}\")\n",
    "        # print(f\"Number of stocks: {len(current_stocks)}\")\n",
    "        \n",
    "        # Get training data for the selected stocks\n",
    "        pretrain_data = df[(df['date'] >= train_start) & (df['date'] <= train_end) & \n",
    "                          (df['permno'].isin(current_stocks))]\n",
    "        drmv_weights = run_single_backtest_select_stocks(\n",
    "            training_data=pretrain_data,\n",
    "            selected_perms=current_stocks,\n",
    "            annual_target_return=0.105,\n",
    "            r=r)\n",
    "        length = len(pretrain_data) / len(current_stocks)\n",
    "        prev_sigma_start_dt = (train_start - relativedelta(months=1))\n",
    "        #prev_sigma_start_dt = (train_start - relativedelta(years=1))\n",
    "        to_get_B_data_intraday = df_intra[(df_intra['date'] >= prev_sigma_start_dt) & (df_intra['date'] <= train_end) & \n",
    "                          (df_intra['permno'].isin(current_stocks))]\n",
    "        matrix, n_to_average = compute_annualized_matrix_type(to_get_B_data_intraday, sigma_real, dt=dt)\n",
    "        ret_matrix = pretrain_data.pivot(index='date', columns='permno', values='ret')\n",
    "        ret_matrix = ret_matrix.fillna(0)\n",
    "        \n",
    "       \n",
    "        # use real sigma matrix (already annualized)\n",
    "        sigma_mat = sigma_real #np.linalg.cholesky(cov)\n",
    "        curr_data = df[(df['date'] <= test_end) & (df['date'] >= current_month)&(df['permno'].isin(current_stocks))]\n",
    "        curr_data_intra = df_intra[(df_intra['date'] <= test_end) & (df_intra['date'] >= current_month)&(df_intra['permno'].isin(current_stocks))]\n",
    "        # dt = 1/length\n",
    "        t_list = np.linspace(0, 1, int(1/dt))\n",
    "        price_st = curr_data.pivot(index='date', columns='permno', values='prc_adjusted').fillna(method='ffill').values\n",
    "        price_st_intra = curr_data_intra.pivot(index=['date', 'hour'], columns='permno', values='prc_adjusted').fillna(method='ffill').values\n",
    "        curr_all_ret = price_st[-1] / price_st[0] - 1\n",
    "        yt = St_to_Yt_vectorized(price_st_intra[np.newaxis, :, :], price_st_intra[0], sigma_mat, r, t_list[1:int(len(curr_data_intra)/num_stocks)+1]) # can be t_list[0:len(curr_data)]\n",
    "        k= solve_k_with_EL(matrix, r=r, sigma=sigma_mat, T=plan_time, beta=beta, num_y=1000, seed=seed)\n",
    "\n",
    "        # calculate radius small delta (using 1 year, represents by T=1)\n",
    "        var = calculate_z_var(T=plan_time, r=r, sigma=sigma_mat, B_support=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)\n",
    "        np.random.seed(seed)\n",
    "        small_delta_array = (np.random.normal(0, np.sqrt(var), size=100)**2)*(calculate_numerator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed)/calculate_denominator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k, seed=seed))\n",
    "        small_delta = np.percentile(small_delta_array, 95)/n_to_average\n",
    "        \n",
    "        # calculate delta_B (using 1 year, represents by T=1)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        delta_B = compute_big_delta_star(matrix, r, plan_time, beta, small_delta, sigma_mat, rng=rng)\n",
    "        \n",
    "        month_r = np.power(1+r, 1/12)-1 \n",
    "        # add month_r to curr_all_ret for drmv\n",
    "        curr_ret_for_drmv = np.append(curr_all_ret, month_r)\n",
    "        daily_kara = 1\n",
    "        daily_drbc = 1\n",
    "        granular_r = np.power(1+r, dt)-1\n",
    "        # last day not trade since no price for next day\n",
    "        for j in range(1,int((curr_data['date'].nunique()/dt)/252)):\n",
    "            kara_frac_daily = pi_fraction_exact(t=j*dt, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat,\n",
    "                        joint_z_vectors=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            drbc_frac_daily = pi_fraction_exact(t=j*dt, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat, \n",
    "                        joint_z_vectors=matrix+delta_B, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            daily_kara *= (1-kara_frac_daily.sum())*granular_r+np.dot(kara_frac_daily, price_st_intra[j] / price_st_intra[j-1] - 1)+1\n",
    "            daily_drbc *= (1-drbc_frac_daily.sum())*granular_r+np.dot(drbc_frac_daily, price_st_intra[j] / price_st_intra[j-1] - 1)+1\n",
    "            \n",
    "        # kara_wealth_list.append((kara_wealth_list[-1]*(1-kara_frac.sum())*month_r+np.dot(kara_frac, curr_all_ret)+1)*kara_wealth_list[-1])\n",
    "        # drbc_wealth_list.append((drbc_wealth_list[-1]*(1-drbc_frac.sum())*month_r+np.dot(drbc_frac, curr_all_ret)+1)*drbc_wealth_list[-1])\n",
    "        drmv_wealth_list.append(drmv_wealth_list[-1]*(1+np.dot(drmv_weights, curr_ret_for_drmv)))\n",
    "        kara_wealth_list.append(daily_kara*kara_wealth_list[-1])\n",
    "        drbc_wealth_list.append(daily_drbc*drbc_wealth_list[-1])\n",
    "\n",
    "    return kara_wealth_list, drbc_wealth_list, drmv_wealth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c71b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially selected stocks: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.0149848763024376 1.0145779288733259\n",
      "2 1.0134495410856965 1.0126736727503591\n",
      "3 0.986527626002127 0.9855646463749171\n",
      "4 0.9853557287737111 0.9835940781522544\n",
      "5 0.981019066152363 0.9792693770556418\n",
      "6 0.9732489857315383 0.9713476725128296\n",
      "7 0.9837319846403157 0.9815945786919802\n",
      "8 0.9913336652958817 0.9914096566805012\n",
      "9 1.0237748425869286 1.021980046356594\n",
      "10 1.02674035602294 1.0248978396619939\n",
      "11 1.0299870465906147 1.0266408309379798\n",
      "12 1.0438615814812158 1.039677462475838\n",
      "13 1.0565373932292839 1.0514017763490642\n",
      "14 1.1116151018203073 1.1057824284027424\n",
      "15 1.0921539419249182 1.0866742507510452\n",
      "16 1.0509189207787581 1.0492966597789315\n",
      "17 1.0819030654162638 1.0806154178571195\n",
      "18 0.973011825117508 0.9727078951860185\n",
      "19 1.0235204335583885 1.0233423400057269\n",
      "20 1.0174396905308574 1.017810749469932\n",
      "21 0.9333543281422333 0.934866343513519\n",
      "22 0.9317521577044978 0.9328175411442011\n",
      "23 0.8993870714714415 0.9008683569698814\n",
      "24 0.9579135130265823 0.9599829425478549\n",
      "25 0.9270444772214522 0.9276133657393203\n",
      "26 0.9279585315966725 0.9282993251027679\n",
      "27 0.9095067169547184 0.9101862745450126\n",
      "28 0.9446576244220204 0.9455887175529413\n",
      "29 1.0055327571010555 1.0063846690219307\n",
      "30 1.0127701404174123 1.0123794422756835\n",
      "31 1.0003587512425844 0.9993095158075225\n",
      "32 1.0331682560355444 1.0344901197383225\n",
      "33 1.0502177633336158 1.051012812735726\n",
      "34 0.9570341641535757 0.9573006413727778\n",
      "35 0.9408554762715298 0.940704095094824\n",
      "36 0.9106856143231299 0.9108786552119849\n",
      "37 0.9013014512979571 0.9017742353718606\n",
      "38 0.8000155426008633 0.7996914039783588\n",
      "39 0.7892524572027076 0.7898844254559689\n",
      "40 0.7920302345572289 0.7919071111684873\n",
      "41 0.875071353927357 0.8765442390561048\n",
      "42 0.826382600152207 0.8286370816742338\n",
      "43 1.016241758648294 1.0211720770637172\n",
      "44 1.2699847876340153 1.2776141371267216\n",
      "45 1.2211833596965982 1.229173127996351\n",
      "46 1.2691638873120554 1.2781449590039442\n",
      "47 1.186295715052638 1.193085438332398\n",
      "48 1.2981551251869647 1.3110914516762215\n",
      "49 1.1589197143112007 1.1653701442587585\n",
      "50 1.0871960818805728 1.0942196979414\n",
      "51 1.0730280758741069 1.081816856675215\n",
      "52 1.177991296862707 1.1902294963623983\n",
      "53 1.2271145294157142 1.239827877826004\n",
      "54 1.1999319475919743 1.2107857267602469\n",
      "55 1.1874416413333178 1.1951040997712872\n",
      "56 1.1183683282804533 1.1266103007576946\n",
      "57 1.0941841970480883 1.1038218396923511\n",
      "58 1.1282060430344847 1.1371757000156337\n",
      "59 1.0911838028720062 1.0981681318721408\n",
      "60 1.2329199926184615 1.2406794382236086\n",
      "61 1.3260563632783928 1.3381192945843827\n",
      "62 1.3080116109260287 1.31381110634125\n",
      "63 1.3927148591635836 1.4021512708216897\n",
      "64 1.3255691581404305 1.333830492046275\n",
      "65 1.3665639220907042 1.3740006582727824\n",
      "66 1.434507628169238 1.4399643528898372\n",
      "67 1.5028143096265332 1.509754316414208\n",
      "68 1.4193406274855809 1.4213021211098196\n",
      "69 1.3402897017896709 1.3373491039153205\n",
      "70 1.265609634202172 1.2613274310813325\n",
      "71 1.5285513344435728 1.5282538287546306\n",
      "72 1.4945343712482257 1.495100183005836\n",
      "73 1.5337332062046438 1.5352639540168194\n",
      "74 1.2412431522839722 1.234031266217986\n",
      "75 1.102154312166198 1.0924024643793544\n",
      "76 1.097861214727863 1.088462707925001\n",
      "77 1.170697550597736 1.1639724129176772\n",
      "78 1.2562466138511024 1.250546736083149\n",
      "79 1.2461519505535097 1.238007056078574\n",
      "80 1.2302969361664875 1.222026113494354\n",
      "81 1.133097615789495 1.125971233187525\n",
      "82 1.1556470759707809 1.144968227824806\n",
      "83 1.1236927629730422 1.114533408163865\n",
      "84 1.093086383233256 1.083744182496183\n",
      "85 1.093380814969315 1.0865933738612101\n",
      "86 0.9477958049237459 0.9376863471109808\n",
      "87 0.8968756185661558 0.8848174142014126\n",
      "88 0.7218195811013255 0.7090670740946116\n",
      "89 0.6681005129096619 0.6550286872504409\n",
      "90 0.6370104264117817 0.6239111711794582\n",
      "91 0.6758782498526554 0.6639694304777185\n",
      "92 0.5933179511025171 0.5820977750635392\n",
      "93 0.5723535931632796 0.5614419459496719\n",
      "94 0.5628916689370278 0.5525762375575206\n",
      "95 0.5401153441332074 0.528138910109084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [07:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mmain_sim_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintraday_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_intraday\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(a[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(a[\u001b[38;5;241m0\u001b[39m][i], a[\u001b[38;5;241m1\u001b[39m][i], a[\u001b[38;5;241m2\u001b[39m][i])\n",
      "Cell \u001b[0;32mIn[113], line 145\u001b[0m, in \u001b[0;36mmain_sim_new\u001b[0;34m(daily_df, intraday_df, r, seed, start_date, end_date, beta, num_stocks, plan_time, dt, sigma_real)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# last day not trade since no price for next day\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mint\u001b[39m((curr_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;241m/\u001b[39mdt)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m252\u001b[39m)):\n\u001b[0;32m--> 145\u001b[0m     kara_frac_daily \u001b[38;5;241m=\u001b[39m \u001b[43mpi_fraction_exact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                \u001b[49m\u001b[43mjoint_z_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_expectation_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     drbc_frac_daily \u001b[38;5;241m=\u001b[39m pi_fraction_exact(t\u001b[38;5;241m=\u001b[39mj\u001b[38;5;241m*\u001b[39mdt, Yt\u001b[38;5;241m=\u001b[39myt[\u001b[38;5;241m0\u001b[39m][j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], T\u001b[38;5;241m=\u001b[39mplan_time, alpha\u001b[38;5;241m=\u001b[39mbeta, r\u001b[38;5;241m=\u001b[39mr, sigma\u001b[38;5;241m=\u001b[39msigma_mat, \n\u001b[1;32m    149\u001b[0m                 joint_z_vectors\u001b[38;5;241m=\u001b[39mmatrix\u001b[38;5;241m+\u001b[39mdelta_B, p_dist\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mones(matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    150\u001b[0m                 num_expectation_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    151\u001b[0m     daily_kara \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mkara_frac_daily\u001b[38;5;241m.\u001b[39msum())\u001b[38;5;241m*\u001b[39mgranular_r\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(kara_frac_daily, price_st_intra[j] \u001b[38;5;241m/\u001b[39m price_st_intra[j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/drmv_drbc/helper.py:300\u001b[0m, in \u001b[0;36mpi_fraction_exact\u001b[0;34m(t, Yt, T, alpha, r, sigma, joint_z_vectors, p_dist, num_expectation_samples, seed)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# 3. 一次性计算所有 y_final_samples 的 F 和 grad_F 的值\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#    这里调用的是我们新的、基于精确求和的函数。\u001b[39;00m\n\u001b[1;32m    295\u001b[0m F_vals \u001b[38;5;241m=\u001b[39m F_tilde_p_exact(\n\u001b[1;32m    296\u001b[0m     t\u001b[38;5;241m=\u001b[39mT, y\u001b[38;5;241m=\u001b[39my_final_samples, joint_z_vectors\u001b[38;5;241m=\u001b[39mjoint_z_vectors, \n\u001b[1;32m    297\u001b[0m     p_dist\u001b[38;5;241m=\u001b[39mp_dist, r\u001b[38;5;241m=\u001b[39mr, sigma\u001b[38;5;241m=\u001b[39msigma\n\u001b[1;32m    298\u001b[0m ) \u001b[38;5;66;03m# 返回形状 (num_expectation_samples,)\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m grad_F_vals \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_F_tilde_p_exact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_final_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_z_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoint_z_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 返回形状 (num_expectation_samples, dim)\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# 4. 计算分子向量 (这部分逻辑不变)\u001b[39;00m\n\u001b[1;32m    306\u001b[0m F_powered_num \u001b[38;5;241m=\u001b[39m F_vals \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (alpha \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha))\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/drmv_drbc/helper.py:146\u001b[0m, in \u001b[0;36mgrad_F_tilde_p_exact\u001b[0;34m(t, y, joint_z_vectors, p_dist, r, sigma)\u001b[0m\n\u001b[1;32m    143\u001b[0m A \u001b[38;5;241m=\u001b[39m (joint_z_vectors \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m@\u001b[39m sigma_inv  \u001b[38;5;66;03m# 形状 (m, dim)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# 2. 计算 L_t 值\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m l_values \u001b[38;5;241m=\u001b[39m \u001b[43mL_t_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_z_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 形状 (m, n_y)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# 3. 计算梯度的期望 E_P[L_t * A]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# 我们需要为每个 y_j 计算 sum_i [p_dist[i] * l_values[i, j] * A[i, :]]\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 这可以通过一个高效的矩阵乘法完成。\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# 首先，用概率对L值进行加权\u001b[39;00m\n\u001b[1;32m    152\u001b[0m weighted_l_values \u001b[38;5;241m=\u001b[39m l_values \u001b[38;5;241m*\u001b[39m p_dist[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;66;03m# 形状 (m, n_y)\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/drmv_drbc/helper.py:87\u001b[0m, in \u001b[0;36mL_t_vectorized\u001b[0;34m(t, z, y, r, sigma)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exponent\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# Percentiles are meaningful only for >1 sample\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     p2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(exponent, \u001b[38;5;241m5\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 87\u001b[0m     p98 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Create a mask for values within the percentile range\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (exponent \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p2) \u001b[38;5;241m&\u001b[39m (exponent \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m p98)\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/conda_envs/tabsyn/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:4287\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, weights, interpolation)\u001b[0m\n\u001b[1;32m   4284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(weights \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   4285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights must be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4288\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/conda_envs/tabsyn/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:4676\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, weights)\u001b[0m\n\u001b[1;32m   4667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[1;32m   4668\u001b[0m                         q,\n\u001b[1;32m   4669\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4673\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   4674\u001b[0m                         weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4675\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4677\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4678\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4679\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4680\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4681\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4682\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4683\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4684\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/conda_envs/tabsyn/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3764\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3761\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[1;32m   3762\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[0;32m-> 3764\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/conda_envs/tabsyn/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:4853\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, weights, axis, out, overwrite_input, method)\u001b[0m\n\u001b[1;32m   4851\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   4852\u001b[0m         wgt \u001b[38;5;241m=\u001b[39m weights\n\u001b[0;32m-> 4853\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4856\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4858\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/gpfs/data/wulab/jiayi/conda_envs/tabsyn/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:4968\u001b[0m, in \u001b[0;36m_quantile\u001b[0;34m(arr, quantiles, axis, method, out, weights)\u001b[0m\n\u001b[1;32m   4964\u001b[0m previous_indexes, next_indexes \u001b[38;5;241m=\u001b[39m _get_indexes(arr,\n\u001b[1;32m   4965\u001b[0m                                               virtual_indexes,\n\u001b[1;32m   4966\u001b[0m                                               values_count)\n\u001b[1;32m   4967\u001b[0m \u001b[38;5;66;03m# --- Sorting\u001b[39;00m\n\u001b[0;32m-> 4968\u001b[0m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4970\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprevious_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4971\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnext_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4972\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4973\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supports_nans:\n\u001b[1;32m   4975\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(arr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = main_sim_new_hour(daily_df=df_day, intraday_df=df_intraday)\n",
    "for i in range(len(a[0])):\n",
    "    print(a[0][i], a[1][i], a[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f83cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sim_new(input_df, r=0.02, seed=42, start_date='2024-01-01', end_date='2024-12-31',\n",
    "                 beta=-3, num_stocks=20, plan_time=1/12):\n",
    "    # Step 1: Load data\n",
    "    \n",
    "    df = input_df\n",
    "    \n",
    "    # Sort by permno and date to ensure proper ordering for log return calculation\n",
    "    df = df.sort_values(['permno', 'date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Calculate log returns for each stock\n",
    "    \n",
    "    \n",
    "    # Step 1: Find stocks with complete data from 2005-12-31 to 2015-01-01\n",
    "    initial_start = pd.to_datetime(start_date) - relativedelta(years=10)    \n",
    "    initial_end = pd.to_datetime(start_date) - pd.Timedelta(days=1)\n",
    "    \n",
    "    # Get stocks that have data in the initial period\n",
    "    initial_period_data = df[(df['date'] >= initial_start) & (df['date'] <= initial_end)]\n",
    "    \n",
    "    # Count trading days in the initial period for validation\n",
    "    total_trading_days = initial_period_data['date'].nunique()\n",
    "    # print(f\"Total trading days in initial period: {total_trading_days}\")\n",
    "    \n",
    "    # Find stocks with sufficient data coverage (at least 80% of trading days)\n",
    "    stock_coverage = initial_period_data.groupby('permno')['date'].nunique()\n",
    "    min_required_days = int(total_trading_days)  # Require at least 80% coverage\n",
    "    valid_stocks_initial = stock_coverage[stock_coverage >= min_required_days].index.tolist()\n",
    "    \n",
    "    # print(f\"Stocks with sufficient data in initial period: {len(valid_stocks_initial)}\")\n",
    "    \n",
    "    # Sample num_stocks stocks from those with complete initial data\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "    selected_stocks = np.sort(np.random.choice(valid_stocks_initial, num_stocks, replace=False))\n",
    "    \n",
    "    print(f\"Initially selected stocks: {selected_stocks}\")\n",
    "    \n",
    "    # Step 2: Process monthly data starting from 2015-01-01\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    month_starts = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "    \n",
    "    current_stocks = selected_stocks.copy()\n",
    "    kara_wealth_list = [1]\n",
    "    drbc_wealth_list = [1]\n",
    "    drmv_wealth_list = [1]\n",
    "    for i, current_month in enumerate(tqdm(month_starts)):\n",
    "        # print(f\"\\nProcessing month {i+1}/{len(month_starts)}: {current_month}\")\n",
    "        \n",
    "        # Define time windows\n",
    "        train_start = current_month - relativedelta(years=10)\n",
    "        train_end = current_month - pd.Timedelta(days=1)\n",
    "        test_start = current_month\n",
    "        test_end = month_starts[i+1] - pd.Timedelta(days=1) if i+1 < len(month_starts) else pd.to_datetime(end_date)\n",
    "        \n",
    "        # print(f\"Previous 10 years: {train_start.date()} to {train_end.date()}\")\n",
    "        # print(f\"Next month: {test_start.date()} to {test_end.date()}\")\n",
    "        \n",
    "        prev_to_next_dates = df[(df['date'] >= train_start) & (df['date'] <= test_end)]['date'].nunique()\n",
    "        \n",
    "        # Efficiently find all stocks with 100% coverage using vectorized operations\n",
    "        # Get data for the entire period (train + test)\n",
    "        full_period_start = train_start\n",
    "        full_period_end = test_end\n",
    "        full_period_data = df[(df['date'] >= full_period_start) & (df['date'] <= full_period_end)]\n",
    "        full_period_stock_dates = full_period_data.groupby('permno')['date'].nunique()\n",
    "        all_valid_stocks = full_period_stock_dates[full_period_stock_dates >= prev_to_next_dates].index.values\n",
    "        \n",
    "        # Check which current stocks are still valid\n",
    "        valid_current_stocks = np.intersect1d(current_stocks, all_valid_stocks)\n",
    "        \n",
    "        # print(f\"Current stocks with 100% coverage: {len(valid_current_stocks)} out of {len(current_stocks)}\")\n",
    "        # print(f\"Total stocks available with 100% coverage: {len(all_valid_stocks)}\")\n",
    "        \n",
    "        # If we need to replace stocks to maintain 20 stocks\n",
    "        stocks_needed = num_stocks - len(valid_current_stocks)\n",
    "        \n",
    "        if stocks_needed > 0:\n",
    "            # print(f\"Need to find {stocks_needed} replacement stocks\")\n",
    "            \n",
    "            # Find replacement candidates (exclude currently valid stocks)\n",
    "            replacement_candidates = np.setdiff1d(all_valid_stocks, valid_current_stocks)\n",
    "            \n",
    "            # print(f\"Available replacement candidates: {len(replacement_candidates)}\")\n",
    "            # add to 20 stocks\n",
    "            stocks_to_add = np.random.choice(replacement_candidates, stocks_needed, replace=False)\n",
    "            # Use all available replacements, even if less than needed\n",
    "            current_stocks = np.sort(np.concatenate([valid_current_stocks, stocks_to_add]))\n",
    "\n",
    "        else:\n",
    "            current_stocks = valid_current_stocks\n",
    "            # print(\"All current stocks are valid, no replacement needed\")\n",
    "        \n",
    "        # print(f\"Final stock selection for this month: {current_stocks}\")\n",
    "        # print(f\"Number of stocks: {len(current_stocks)}\")\n",
    "        \n",
    "        # Get training data for the selected stocks\n",
    "        pretrain_data = df[(df['date'] >= train_start) & (df['date'] <= train_end) & \n",
    "                          (df['permno'].isin(current_stocks))]\n",
    "        drmv_weights = run_single_backtest_select_stocks(\n",
    "            training_data=pretrain_data,\n",
    "            selected_perms=current_stocks,\n",
    "            annual_target_return=0.105,\n",
    "            r=r)\n",
    "        length = len(pretrain_data) / len(current_stocks)\n",
    "        prev_sigma_start_dt = (train_start - relativedelta(months=1))\n",
    "        #prev_sigma_start_dt = (train_start - relativedelta(years=1))\n",
    "        to_get_B_data = df[(df['date'] >= prev_sigma_start_dt) & (df['date'] <= train_end) & \n",
    "                          (df['permno'].isin(current_stocks))]\n",
    "        matrix, _ = compute_annualized_matrix_type(to_get_B_data, sigma_real)\n",
    "        ret_matrix = pretrain_data.pivot(index='date', columns='permno', values='ret')\n",
    "        ret_matrix = ret_matrix.fillna(0)\n",
    "        \n",
    "       \n",
    "        # use real sigma matrix (already annualized)\n",
    "        sigma_mat = sigma_real #np.linalg.cholesky(cov)\n",
    "        curr_data = df[(df['date'] <= test_end) & (df['date'] >= current_month)&(df['permno'].isin(current_stocks))]\n",
    "        dt = 1/length\n",
    "        t_list = np.linspace(0, 1, 252)\n",
    "        price_st = curr_data.pivot(index='date', columns='permno', values='prc_adjusted').fillna(method='ffill').values\n",
    "        curr_all_ret = price_st[-1] / price_st[0] - 1\n",
    "        yt = St_to_Yt_vectorized(price_st[np.newaxis, :, :], price_st[0], sigma_mat, r, t_list[1:int(len(curr_data)/20)+1]) # can be t_list[0:len(curr_data)]\n",
    "        k= solve_k_with_EL(matrix, r=r, sigma=sigma_mat, T=plan_time, beta=beta, num_y=1000, seed=seed)\n",
    "\n",
    "        # calculate radius small delta (using 1 year, represents by T=1)\n",
    "        var = calculate_z_var(T=plan_time, r=r, sigma=sigma_mat, B_support=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k)\n",
    "        small_delta_array = (np.random.normal(0, np.sqrt(var), size=100)**2)*(calculate_numerator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k)/calculate_denominator(plan_time, r, sigma_mat, matrix, np.ones(matrix.shape[0])/matrix.shape[0], beta=beta, k=k))\n",
    "        small_delta = np.percentile(small_delta_array, 95)/40\n",
    "        \n",
    "        # calculate delta_B (using 1 year, represents by T=1)\n",
    "        delta_B = compute_big_delta_star(matrix, r, plan_time, beta, small_delta, sigma_mat)\n",
    "        \n",
    "        month_r = np.power(1+r, 1/12)-1 \n",
    "        # add month_r to curr_all_ret for drmv\n",
    "        curr_ret_for_drmv = np.append(curr_all_ret, month_r)\n",
    "        daily_kara = 1\n",
    "        daily_drbc = 1\n",
    "        daily_r = np.power(1+r, 1/252)-1\n",
    "        # last day not trade since no price for next day\n",
    "        for j in range(1,curr_data['date'].nunique()):\n",
    "            kara_frac_daily = pi_fraction_exact(t=j/252, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat,\n",
    "                        joint_z_vectors=matrix, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            drbc_frac_daily = pi_fraction_exact(t=j/252, Yt=yt[0][j-1], T=plan_time, alpha=beta, r=r, sigma=sigma_mat, \n",
    "                        joint_z_vectors=matrix+delta_B, p_dist=np.ones(matrix.shape[0])/matrix.shape[0],\n",
    "                        num_expectation_samples=5000, seed=seed)\n",
    "            daily_kara *= (1-kara_frac_daily.sum())*daily_r+np.dot(kara_frac_daily, price_st[j] / price_st[j-1] - 1)+1\n",
    "            daily_drbc *= (1-drbc_frac_daily.sum())*daily_r+np.dot(drbc_frac_daily, price_st[j] / price_st[j-1] - 1)+1\n",
    "            \n",
    "            \n",
    "        # kara_wealth_list.append((kara_wealth_list[-1]*(1-kara_frac.sum())*month_r+np.dot(kara_frac, curr_all_ret)+1)*kara_wealth_list[-1])\n",
    "        # drbc_wealth_list.append((drbc_wealth_list[-1]*(1-drbc_frac.sum())*month_r+np.dot(drbc_frac, curr_all_ret)+1)*drbc_wealth_list[-1])\n",
    "        drmv_wealth_list.append(drmv_wealth_list[-1]*(1+np.dot(drmv_weights, curr_ret_for_drmv)))\n",
    "        kara_wealth_list.append(daily_kara*kara_wealth_list[-1])\n",
    "        drbc_wealth_list.append(daily_drbc*drbc_wealth_list[-1])\n",
    "\n",
    "    return kara_wealth_list, drbc_wealth_list, drmv_wealth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "88e37f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially selected stocks: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:37<00:00,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "b = main_sim_new(sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a89b9206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "1.131022826203763 1.1050645848799752 1.012418018656068\n",
      "1.2144529070425427 1.1601536438012992 1.0257958812660561\n",
      "1.089108260659705 1.0749891426547524 1.0143646161930415\n",
      "1.1629329160342259 1.1488161486473039 1.0187401561473046\n",
      "1.0970245458996184 1.0928991032635385 1.018295694471291\n",
      "1.1831434925593105 1.170406270711924 1.0303369712956925\n",
      "1.2940546736530993 1.2671131232395831 1.0356019472961155\n",
      "1.3611240772687132 1.303262263543781 1.045469085209132\n",
      "1.5260745591506752 1.4348467258983542 1.0629519000104395\n",
      "1.531003060006961 1.4401413646006256 1.0690385648874023\n",
      "1.818748843280903 1.7054870553769472 1.084657178877144\n",
      "1.545333001829128 1.519724143201071 1.0720840286712676\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(b[0])):\n",
    "    print(b[0][i], b[1][i], b[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b7cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
